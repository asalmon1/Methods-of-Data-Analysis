---
title: 'STOR 455 Homework #5'
subtitle: 40 points - Due Wednesday 3/20 at 5:00pm
geometry: margin = 2.25cm
output:
  pdf_document: default
---

__Directions:__ For parts 7 and 10 you should work together, but these parts must be __submitted individually__ by each group member. For parts 8 and 9, you must have only __one submission per group__. There will be separate places on Gradescope to submit the individual vs group work. 

__Situation:__ Can we predict the selling price of a house in Ames, Iowa based on recorded features of the house? That is your task for this assignment. Each team will get a dataset with information on forty potential predictors and the selling price (in $1,000’s) for a sample of homes. The data sets for your group are AmesTrain??.csv and AmesTest??.csv (where ?? corresponds to your group number) A separate file identifies the variables in the Ames Housing data and explains some of the coding.

#### Part 7. Cross-validation: ####
In some situations, a model might fit the peculiarities of a specific sample of data well, but not reflect structure that is really present in the population. A good test for how your model might work on "real" house prices can be simulated by seeing how well your fitted model does at predicting prices that were NOT in your original sample. This is why we reserved an additional 200 cases as a holdout sample in AmesTest??.csv. Use the group number and AmesTest??.csv corresponding to your group number for homework #3. Import your holdout test data and 

```{r}
library(tidyverse)
library(car)
library(corrplot)
library(leaps)
source("https://raw.githubusercontent.com/JA-McLean/STOR455/master/scripts/ShowSubsets.R")

columns_to_exclude <- c("Order")
HouseData <- read_csv('AmesTrain3.csv', show_col_types = FALSE)
newHouseData <- HouseData[, !names(HouseData) %in% columns_to_exclude]
indices <- c(343, 222, 109, 78, 319, 380, 292, 351, 588, 179)
HouseData1 <- newHouseData %>%
  slice(-indices)

mod1 <- lm(formula = Price ~ Quality + GroundSF + BasementFinSF + BasementSF +
             LotArea + YearBuilt + GarageSF + YearRemodel + Bedroom + 
             LotFrontage + FullBath + Condition, data = HouseData1)
```

```{r}
TestData <- read_csv('AmesTest3.csv', show_col_types = FALSE)

TestData$TotalPorchSF <- TestData$OpenPorchSF + TestData$EnclosedPorchSF + TestData$ScreenPorchSF
```

* Compute the predicted Price for each of the cases in the holdout test sample, using your model resulting from the initial fit and residual analysis in parts 1 through 3 of Homework #3. 

```{r}
prediction <- predict(mod1, newdata = TestData)
head(prediction)
```

Above, I have recorded the predictions for the first six cases in the holdout sample. This is in terms of Price, which is in thousands of dollars (so 247.0232 = $247,023.20)

* Compute the residuals for the 200 holdout cases.

```{r}
resids <- TestData$Price - prediction
head(resids)
```

Above, I have recorded the residuals for the first six cases in the holdout sample. Again, these are in terms of thousands of dollars.

* Compute the mean and standard deviation of these residuals. Are they close to what you expect from the training model?

```{r}
mean(resids)
sd(resids)
```

Here, the mean is -1.613542 and the standard deviation is 31.65448. In terms of the dollar amount, this is a mean of -$1,613.54, approximately. Overall, this is probably around what I expected from this model. The standard deviation is not that large, and the mean is pretty close to 0, when we consider the scale that this is in (thousands of dollars). Realistically, this standard deviation is not bad in the context of the data. Overall, this model is not perfect (it may be overpredicting slightly, as the mean is negative) but it is sufficient.

* Construct a plot of the residuals to determine if they are normally distributed. Is this plot what you expect to see considering the training model?

```{r}
qqnorm(resids)
qqline(resids)
```

This plot indicates that the residuals have a normal distribution, with longer tails than expected. It does not appear to be skewed. This makes sense overall, since with house prices there are probably going to be some extremes on both ends.

* Are any holdout cases especially poorly predicted by the training model? If so, identify by the row number(s) in the holdout data. Why might these cases be poorly predicted?

```{r}
sort(abs(resids), decreasing = TRUE)[1:10]
```
These are the largest 10 residuals, in terms of thousands of dollars. So, the residual with the largest magnitude tells me that, for one particular value in the holdout data, my model was off by (a magnitude of) $233,878.62. This is a very large residual, as are the next several, which are all (besides the 10th) larger than \$50,000. Personally, I know that if I found out a house I wanted to buy was \$233,878.6 more expensive than expected, I would be pretty mad! There isn't a very easy way to determine why these residuals are so large, besides examining them on a case-by-case basis. However, I know that there are many factors that go into determining the price of a house. Some of those factors may be present in the dataframe, but not in my model. Additionally, some factors are not in the dataframe either. For example, location of a house makes a significant impact on its price, but there is no column for location in the dataframe.

* Compute the correlation between the predicted values and actual prices for the holdout sample. This is known as the cross-validation correlation. We don’t expect the training model to do better at predicting values different from those that were used to build it (as reflected in the original $R^{2}$), but an effective model shouldn’t do a lot worse at predicting the holdout values. Square the cross-validation correlation to get an $R^{2}$ value and subtract it from the original multiple $R^{2}$ of the training sample. This is known as the shrinkage. We won’t have specific rules about how little the shrinkage should be, but give an opinion on whether the shrinkage looks OK to you or too large in your situation. 

```{r}
crosscorr <- cor(TestData$Price, prediction)

shrinkage <- summary(mod1)$r.squared - crosscorr^2
shrinkage
```

In my opinion, this shrinkage indicates that my model was effective for predicting price in the holdout sample. A shrinkage of 0.0916 means that the $R^{2}$ (proportion of variability in the data explained by the model) of the holdout data is 0.0916 less than the $R^{2}$ of the training data. I think that this is a pretty low shrinkage overall, so it indicates that I have a good model.

#### Part 8. Find a “fancy model”: #### 
Again using AmesTrain??.csv, where ?? corresponds to your new group number in homework #5, to build a regression model to predict Price. In addition to the quantitative predictors, you may now consider models with

* Categorical variables - Just put these in the model and let R take care of making the indicator predictors (and picking one category to leave out). Use factor( ) to treat a numeric variable as categorical. You’ll see the coefficients for each indicator when you look at the summary( ) and they will be grouped together in the ANOVA. Be careful, since adding a single categorical variable with a lot of categories might actually be adding a lot of new indicator terms.

* Transformations of predictors -  You can include functions of quantitative predictors. Probably best to use the I( ) notation so you don’t need to create new columns when you run the predictions for the test data. 

* Transformations of the response - You might address curvature or skewness in residual plots by transforming the response prices with a function like log(Price), sqrt(Price), Price^2, etc..  These should generally not need the I( ) notation to make these adjustments. IMPORTANT: If you transform Price, be sure to reverse the transformation when making final predictions!

* Combinations of variables - This might include interactions or other combinations. You do not need the I( ) notation when making an interaction using a categorical predictor (e.g.  GroundSF*CentralAir).

Keep general track of the approaches you try and explain what guides your decisions as you select a new set of predictors (but again you don’t need to give full details of every model you consider). Along the way you should consider some residual analysis. 

Notes/Tips:

* WARNING: When using a categorical predictor with multiple categories in regsubsets( ), R will create indicators and treat them as separate predictors when deciding which to put into a model. So you might get a model with quantitative predictors like LotArea and GroundSF along with specific indicators like GarageQTA and HouseStyle1Story. This may not be very useful, since we should generally use all indicators for a categorical predictor if we include one in the model. On the other hand, when using the step( ) function, R will generally keep the multiple indicators for different categories of the same variable together as a unit. 

* In some cases the indicators created for different categorical variables will have identical values.  For example, if you include both GarageC and GarageQ in a model, R will produce values for each of the indicators. The indicators for GarageQNone and GarageCNone (equal to one only for houses that don’t have a garage) will be identical. This may be handled differently in R depending on the procedure. regsubsets( ) may give a “warning” about variables being linearly dependent.  You can still use the results, just be aware that some variables are completely dependent. lm( ) might give output with coefficients (and tests) of some predictors listed as NA.  This is not a problem, R is just automatically deleting one of the redundant variables. If you are predicting for a house with no garage you might have a coefficient to use for GarageQNone but then you don’t need to worry about having one for GarageCNone.

* If your residual analysis from homework #3 or an early model here suggest you might want to do a transformation for the response variable (Price), do so _before_ fitting a lot more models. No sense fine tuning a set of predictors for Price, then deciding you should be predicting log(Price) or Price^2. So make that decision fairly early, but don’t get too picky and expect to get perfect plot of residuals versus fits or an exact normal quantile plot.

* Similarly, if you decide that some data cases should be dropped from the training set, don’t wait until late in the process to do so. For example, if you spot a _very_ large residual you should look at the characteristics for that house to see if it should be deleted. Don’t forget about the value of simple plots (like a scatterplot of Price vs. LotArea) for helping to see what is going on and recognize extreme cases. Be sure to document any adjustments you make in the final report. 

* Comparing $C_{p}$ from different predictor pools - While Mallow’s $C_{p}$ is a useful tool for comparing models from the same pool of predictors. You should not use it to compare models based on different predictor pools. For example, if you add a bunch of categorical variables to all the quantitative predictors from homework #3 to make a new “full” model, then find $C_{p}$ from a model that you fit in homework #3, it will be worse than it was before. If you look at the formula for calculating $C_{p}$, you will see that all that has changed is MSE for the full model after adding the new batch of predictors.  

* I should be able to follow the steps you use when selecting a model. I certainly don’t need to see every bit of output, but it might help to include more of the R commands you use. For example, saying you used backward elimination is not very helpful when I don’t know what you start with for the full model or pool of predictors (e.g. did you include Condition and Quality as numeric predictors? or did you decide to eliminate one of GroundSF, FirstSF, or SecondSF due to redundancy?). The easiest way to convey this in many cases is to show the R command you used. It is fine to abbreviate the output (for example, delete many steps in a stepwise procedure using trace=FALSE), but it would be helpful if you identified the parts you do include.  For example, a sentence like “After 12 steps of the stepwise procedure, we have the output below for the fitted model.”  Similarly, I don’t need to see 600 residuals, using head and sort can show the important ones.

* Once you have settled on a response, made adjustments to the data (if needed), and chosen a set of predictors, be sure to include the summary( ) for your “fancy” model at this stage. 

So, before starting, we decided to take the log() of Price, as in the previous assignment we felt that it helped a great deal with our model. Additionally, if you look at scatterplots for the quantitative variables, many of them appear to have an exponential relationship with Price, further indicating that this transformation is appropriate.

```{r}
# Fit the full model
Full <- lm(log(Price) ~ ., data = newHouseData)
# Find the MSE for the full model
MSE <- (summary(Full)$sigma)^2
# Start with a model with NO predictors
none <- lm(log(Price) ~ 1, data = newHouseData)
# Don’t specify a direction
step(none, scope = list(upper = Full), scale = MSE, trace = FALSE)
```

So, we started off using stepwise regression, this time including categorical variables. We ended up with a pretty large model, with many predictors, so we thought we would try to reduce the size of the model.

```{r}
model1 <- lm(formula = log(Price) ~ Quality + GroundSF + BasementFinSF + 
    YearBuilt + Condition + BasementSF + LotArea + ExteriorC + 
    BasementC + ExteriorQ + GarageType + GarageC + Foundation + 
    GarageCars + Fireplaces + EnclosedPorchSF + OpenPorchSF + 
    CentralAir + Heating + FullBath + LotFrontage + WoodDeckSF + 
    ScreenPorchSF + SecondSF, data = HouseData1)
#summary(model1)

newHouseData$TotalPorchSF <- newHouseData$OpenPorchSF + HouseData$EnclosedPorchSF + newHouseData$ScreenPorchSF
```

We noticed that our model determined by the stepwise regression included all three variables (EnclosedPorchSF, OpenPorchSF, and ScreenPorchSF) for porch square footage. We decided to simplify this addition by combining them into one singular variable, TotalPorchSF. When we made this change to the model, the $R^{2}$ remained almost exactly the same, indicating that this was a good way to simplify the model. (Note that we chose to differentiate between a deck and a porch).

```{r}
model2 <- lm(log(Price) ~ Quality + GroundSF + BasementFinSF + 
    YearBuilt + Condition + BasementSF + LotArea + ExteriorC + 
    BasementC + ExteriorQ + GarageC + Foundation + 
    GarageCars + Fireplaces + TotalPorchSF + CentralAir + 
    Heating + FullBath + LotFrontage + WoodDeckSF + SecondSF, 
    data = newHouseData)
#summary(model2)
```

We also noticed that, in the original model from the stepwise regression, there were two variables regarding the garage: GarageC and GarageType. However, only two of the GarageType dummy variables were statistically significant according to the individual t-tests in the summary output, CarPort and None. None was extremely significant, while CarPort was only somewhat significant (with a p-value around 0.023). So it seemed that only two or three pieces of information from the GarageType variable were making a noticeable difference in our model - whether the garage had a carport, possibly the one dummy variable not included in the model, and whether there was a garage at all. However, all four dummy variables for GarageC were significant based on their individual t-tests (except for GarageCNone, since that was not included in the output). Clearly, GarageC was providing more important information to the model than GarageType, and the information being provided by GarageType that was most relavant to the model was also being provided by GarageC - in this case, whether or not there was a garage! (GarageCNone and GarageTypeNone are exactly the same). So, we ultimately decided to delete GarageType as a predictor variable for this reason.

```{r}
model2_reduced <- lm(formula = log(Price) ~ Quality + GroundSF + BasementFinSF + 
    YearBuilt + Condition + BasementSF + LotArea + ExteriorC + 
    BasementC + ExteriorQ + GarageC + Foundation + GarageCars + 
    Fireplaces + TotalPorchSF + CentralAir + Heating + 
    LotFrontage + WoodDeckSF, data = newHouseData)

anova(model2, model2_reduced)
```
After making the above edits to our model, we again looked at the summary output, and noticed that the FullBath and SecondSF variables were not significant at a 0.05 level based on their individual t-tests. So, we decided to remove these from the model to create a reduced model, and then use a nested F-test to determine whether there was a significant difference between the full and reduced models. We took H0 = the reduced model, and HA = the full model. This resulted in a 0.06829 p-value in our F-test, which is greater than 0.05 and thus we failed to reject the null and kept our reduced model.

```{r}
finalmodel <- model2_reduced
summary(finalmodel)
```
So, our process resulted in this final model, which includes log(Price) as the response variable, both categorical and quantitative predictor variables, and the predictor variable TotalPorchSF that we created by combining three related variables. Overall, we believe this is a good model. It has an adjusted $R^{2}$ of 0.8982 and an $R^{2}$ of 0.9043 indicating that this model predicts a major proportion of the variability in the data. All of the predictors are significant at a 0.05 level based on the individual t-tests. While some of the dummy variables for quantitative predictors are not significant at the 0.05 level, every quantitative predictor has at least one dummy variable that is statistically significant based on its t-test. This indicates that every predictor in this model is providing important information that improves its effectiveness.

#### Part 9: Cross-validation for your “fancy” model ####
    
Redo the cross-validation analysis with your test data for your new fancy model. Use AmesTest??.csv, where ?? corresponds to your new group number for homework #5. Discuss how the various measures (mean of residuals, std. dev of residuals, shape of the distributions of residuals, cross-validation correlation, and shrinkage) compare to the results you had for your basic model.  Don’t worry about looking for poorly predicted cases this time. If you transformed the response variable, consider how to take this into account for your residual analysis. In order to compare residuals they should have the same units!

First, we would like to note that we encountered two issues when trying to apply the model to our holdout data. First, there was one house in the holdout data that has values of 0 rather than None for categories like BasementC and BasementType. Since the various variables for the square footage of a house's basement were all also equal to 0, we concluded that this house clearly just did not have a basement. Thus, we changed the appropriate values to None.

Second, one house in the holdout data had a value of "Ex," meaning excellent, for the GarageC category. This raised an error, as no houses in our training data had this rating. We decided the best solution was to change this value to "Gd," meaning good.

```{r}

TestData$BasementC <- ifelse(TestData$BasementC == 0, "None", TestData$BasementC)
TestData$GarageC <- ifelse(TestData$GarageC == "Ex", "Gd", TestData$GarageC)

logprediction <- predict(finalmodel, newdata = TestData)
finalprediction <- exp(logprediction)

finalresids <- TestData$Price - finalprediction
head(finalresids)
```

I would also like to note that I used the exp() function to get the predicted values in terms of Price rather than log(Price). Above are the first few residual values for the model on the holdout data.

```{r}
mean(finalresids)
sd(finalresids)
```

The mean and standard deviation of the residuals indicate that this model is better than the one I discussed in Part 7. The mean is very close to 0. It is in terms of thousands of dollars, so the mean is $438.12, which when we are talking about houses that cost hundreds of thousands of dollars is relatively very close to 0. The standard deviation is also a little smaller than the standard deviation in Part 7, which was 31.65448. Thus, this suggests that this model is an improvement on my first model.

```{r}
qqnorm(finalresids)
qqline(finalresids)
```

Similarly to the model in Part 7, the residuals appear to be normally distributed in the center of the data, but with long tails on both ends. So, there were a few values with unusually large or small residuals.

```{r}
crosscorr1 <- cor(TestData$Price, finalprediction)

shrinkage <- summary(finalmodel)$r.squared - crosscorr1^2
shrinkage
```

Finally, we have a shrinkage of 0.05413893. This is an excellent shrinkage, as the $R^{2}$ did not decrease much at all when applying the model to the holdout data.

Note on missing categories:

>When creating the predictions using predict(yourmodel,AmesTest) you may see an error like:  

>Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) :   factor HouseStyle has new levels 1.5Unf, 2.5Fin, 2.5Unf  
  
>This occurs because the holdout sample has a value for the categorical variable that was not present in your training sample, so there is no indicator in your model to handle that case. To get a prediction for that house, you’ll need to switch the category to one that is in your training data.  In the example above you might choose to replace the “2.5Fin” house style with “2Story”. If you are not sure what category to use, try whatever R uses as the “left out” reference category. Be sure to record any changes like this that you make.

#### Part 10. Final Model ####  

Again, you may choose to make some additional adjustments to your model after considering the final residual analysis. If you do so, please explain what (and why) you did and provide the summary() for your new final model.
    
Suppose that you are interested in a house in Ames that has the characteristics listed below. Construct a 95% confidence interval for the mean price of such houses.

A 2 story 11 room home, built in 1987 and remodeled in 1999 on a 21540 sq. ft. lot with 328 feet of road frontage. Overall quality is good (7) and condition is average (5). The quality and condition of the exterior are both good (Gd) and it has a poured concrete foundation. There is an 757 sq. foot basement that has excellent height, but is completely unfinished and has no bath facilities. Heating comes from a gas air furnace that is in excellent condition and there is central air conditioning. The house has 2432 sq. ft. of living space above ground, 1485 on the first floor and 947 on the second, with 4 bedrooms, 2 full and one half baths, and 1 fireplace. The 2 car, built-in garage has 588 sq. ft. of space and is average (TA) for both quality and construction. The only porches or decks is a 205 sq. ft. open porch in the front. 

```{r}
my_house <- data.frame(Quality = 7, GroundSF = 2432, BasementFinSF = 0, YearBuilt = 1987, Condition = 5, BasementSF = 757, LotArea = 21540, ExteriorC = "Gd", BasementC = "TA", ExteriorQ = "Gd", GarageC = "TA", Foundation = "PConc", GarageCars = 2, Fireplaces = 1, TotalPorchSF = 205, CentralAir = "Y", Heating = "GasA", LotFrontage = 328, WoodDeckSF = 0)

exp(predict.lm(finalmodel, my_house, interval = "confidence", level = 0.95))
```

I am 95% confident that the true mean price for all houses in Ames with the above characteristics lies between \$242,055.60 and \$301,310.10.

